import{_ as e,c as t,o as r,a4 as a}from"./chunks/framework.BSO0Jayu.js";const g=JSON.parse('{"title":"Why we wrote Lux?","description":"","frontmatter":{},"headers":[],"relativePath":"introduction/overview.md","filePath":"introduction/overview.md","lastUpdated":null}'),s={name:"introduction/overview.md"},o=a('<h1 id="Why-we-wrote-Lux?" tabindex="-1">Why we wrote Lux? <a class="header-anchor" href="#Why-we-wrote-Lux?" aria-label="Permalink to &quot;Why we wrote Lux? {#Why-we-wrote-Lux?}&quot;">â€‹</a></h1><p>Julia already has quite a few well established Neural Network Frameworks â€“ <a href="https://fluxml.ai/" target="_blank" rel="noreferrer">Flux</a> &amp; <a href="https://denizyuret.github.io/Knet.jl/latest/" target="_blank" rel="noreferrer">KNet</a>. However, certain design elements â€“ <strong>Coupled Model and Parameters</strong> &amp; <strong>Internal Mutations</strong> â€“ associated with these frameworks make them less compiler and user friendly. Making changes to address these problems in the respective frameworks would be too disruptive for users. Here comes in <code>Lux</code>: a neural network framework built completely using pure functions to make it both compiler and autodiff friendly.</p><h2 id="Design-Principles" tabindex="-1">Design Principles <a class="header-anchor" href="#Design-Principles" aria-label="Permalink to &quot;Design Principles {#Design-Principles}&quot;">â€‹</a></h2><ul><li><p><strong>Layers must be immutable</strong> â€“ cannot store any parameter/state but rather store the information to construct them</p></li><li><p><strong>Layers are pure functions</strong></p></li><li><p><strong>Layers return a Tuple containing the result and the updated state</strong></p></li><li><p><strong>Given same inputs the outputs must be same</strong> â€“ yes this must hold true even for stochastic functions. Randomness must be controlled using <code>rng</code>s passed in the state.</p></li><li><p><strong>Easily extensible</strong></p></li></ul><h2 id="Why-use-Lux-over-Flux?" tabindex="-1">Why use Lux over Flux? <a class="header-anchor" href="#Why-use-Lux-over-Flux?" aria-label="Permalink to &quot;Why use Lux over Flux? {#Why-use-Lux-over-Flux?}&quot;">â€‹</a></h2><ul><li><p><strong>Neural Networks for SciML</strong>: For SciML Applications (Neural ODEs, Deep Equilibrium Models) solvers typically expect a monolithic parameter vector. Flux enables this via its <code>destructure</code> mechanism, but <code>destructure</code> comes with various <a href="https://fluxml.ai/Optimisers.jl/dev/api/#Optimisers.destructure" target="_blank" rel="noreferrer">edge cases and limitations</a>. Lux forces users to make an explicit distinction between state variables and parameter variables to avoid these issues. Also, it comes battery-included for distributed training using <a href="https://github.com/avik-pal/FluxMPI.jl" target="_blank" rel="noreferrer">FluxMPI.jl</a> <em>(I know ðŸ˜› the naming)</em></p></li><li><p><strong>Sensible display of Custom Layers</strong> â€“ Ever wanted to see Pytorch like Network printouts or wondered how to extend the pretty printing of Flux&#39;s layers? Lux handles all of that by default.</p></li><li><p><strong>Truly immutable models</strong> - No <em>unexpected internal mutations</em> since all layers are implemented as pure functions. All layers are also <em>deterministic</em> given the parameters and state: if a layer is supposed to be stochastic (say <code>Dropout</code>), the state must contain a seed which is then updated after the function call.</p></li><li><p><strong>Easy Parameter Manipulation</strong> â€“ By separating parameter data and layer structures, Lux makes implementing <code>WeightNorm</code>, <code>SpectralNorm</code>, etc. downright trivial. Without this separation, it is much harder to pass such parameters around without mutations which AD systems don&#39;t like.</p></li></ul><h2 id="Why-not-use-Lux?" tabindex="-1">Why not use Lux? <a class="header-anchor" href="#Why-not-use-Lux?" aria-label="Permalink to &quot;Why not use Lux? {#Why-not-use-Lux?}&quot;">â€‹</a></h2><ul><li><p><strong>Small Neural Networks on CPU</strong> â€“ Lux is developed for training large neural networks. For smaller architectures, we recommend using <a href="https://github.com/PumasAI/SimpleChains.jl" target="_blank" rel="noreferrer">SimpleChains.jl</a>.</p></li><li><p><strong>Lux won&#39;t magically speed up your code (yet)</strong> â€“ Lux shares the same backend with Flux and so if your primary desire to shift is driven by performance, you will be disappointed.</p></li><li><p><strong>XLA Support</strong> â€“ Lux doesn&#39;t compile to XLA which means no TPU support unfortunately.</p></li></ul>',8),i=[o];function n(l,u,d,c,h,p){return r(),t("div",null,i)}const f=e(s,[["render",n]]);export{g as __pageData,f as default};
